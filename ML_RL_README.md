# ğŸ¤– Machine Learning & Reinforcement Learning Features

Umfassende ML/RL-Integration fÃ¼r den KI-Trading-Bot.

---

## ğŸ¯ Quick Start

### Installation

```powershell
# Windows (PowerShell)
.\venv\Scripts\python.exe -m pip install -r requirements.txt
```

### Demo ausfÃ¼hren

```powershell
# RL Training Demo
.\scripts\demo_rl.ps1

# Portfolio Optimization Demo
.\scripts\demo_portfolio.ps1

# ML API starten
.\scripts\start_ml_api.ps1
```

---

## ğŸ“ Projekt-Struktur

```
ai.traiding/
â”œâ”€â”€ rl_environment.py          # OpenAI Gym Trading Environment
â”œâ”€â”€ rl_agent.py                # DQN/PPO Agent Wrapper
â”œâ”€â”€ hyperparameter_tuning.py   # Optuna Hyperparameter Tuning
â”œâ”€â”€ portfolio_optimizer.py     # Portfolio Optimization (Markowitz)
â”œâ”€â”€ ml_pipeline.py             # TensorFlow/Keras ML Models
â”œâ”€â”€ ml_api.py                  # Flask API fÃ¼r Model Deployment
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_rl_environment.py     # RL Environment Tests
â”‚   â””â”€â”€ test_portfolio_optimizer.py # Portfolio Tests
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ RL_GUIDE.md                # RL Dokumentation
â”‚   â”œâ”€â”€ ML_GUIDE.md                # ML Dokumentation
â”‚   â””â”€â”€ PORTFOLIO_OPTIMIZATION_GUIDE.md
â”‚
â”œâ”€â”€ demos/
â”‚   â”œâ”€â”€ demo_rl_training.py        # RL Training Demos
â”‚   â””â”€â”€ demo_portfolio_optimization.py
â”‚
â””â”€â”€ scripts/
    â”œâ”€â”€ demo_rl.ps1                # RL Demo Script (Windows)
    â”œâ”€â”€ demo_portfolio.ps1         # Portfolio Demo Script
    â””â”€â”€ start_ml_api.ps1           # ML API Start Script
```

---

## ğŸš€ Features

### 1. Reinforcement Learning

**Environment:**
- âœ… Gym-kompatible Trading-Umgebung
- âœ… State: Preise, Indikatoren, Position, Kapital
- âœ… Actions: BUY/SELL/HOLD mit Mengen
- âœ… Reward: P&L + Sharpe - Drawdown

**Algorithmen:**
- âœ… Deep Q-Network (DQN)
- âœ… Proximal Policy Optimization (PPO)

**Training:**
```python
from rl_agent import train_rl_agent

agent = train_rl_agent(
    algorithm='PPO',
    train_data=data,
    total_timesteps=100000
)
```

**Dokumentation:** [RL_GUIDE.md](RL_GUIDE.md)

---

### 2. Hyperparameter Tuning

**Optuna Integration:**
- âœ… Bayesian Optimization
- âœ… Automatische Hyperparameter-Suche
- âœ… Visualization & Result Persistence

**Beispiel:**
```python
from hyperparameter_tuning import tune_rl_agent

results = tune_rl_agent(
    algorithm='PPO',
    train_data=train_data,
    eval_data=eval_data,
    n_trials=50
)

print(f"Best parameters: {results['best_params']}")
```

---

### 3. Portfolio Optimization

**Methoden:**
- âœ… Markowitz (Maximum Sharpe Ratio)
- âœ… Minimum Volatility
- âœ… Risk Parity
- âœ… Kelly Criterion
- âœ… Dynamic Rebalancing

**Beispiel:**
```python
from portfolio_optimizer import optimize_portfolio

result = optimize_portfolio(
    prices,
    method='max_sharpe',
    risk_free_rate=0.02
)

for asset, weight in result['weights'].items():
    print(f"{asset}: {weight*100:.1f}%")
```

**Dokumentation:** [PORTFOLIO_OPTIMIZATION_GUIDE.md](PORTFOLIO_OPTIMIZATION_GUIDE.md)

---

### 4. ML Pipeline

**Model Types:**
- âœ… Dense (Feed-Forward)
- âœ… LSTM (Time Series)
- âœ… CNN (Pattern Recognition)

**Features:**
- âœ… Signal Prediction (BUY/HOLD/SELL)
- âœ… Model Versioning
- âœ… StandardScaler Integration

**Beispiel:**
```python
from ml_pipeline import TradingMLModel

model = TradingMLModel(input_shape=(10,), model_type='dense')
model.build_model()
model.compile_model()

history = model.train(X_train, y_train, X_val, y_val)
model.save_model('my_model')
```

**Dokumentation:** [ML_GUIDE.md](ML_GUIDE.md)

---

### 5. Flask API

**Endpoints:**
- `GET /health` - Health Check
- `POST /api/ml/load` - ML Modell laden
- `POST /api/ml/predict` - ML Prediction
- `POST /api/rl/load` - RL Agent laden
- `POST /api/rl/predict` - RL Action abrufen
- `GET /api/models/list` - Alle Modelle auflisten

**Starten:**
```powershell
.\scripts\start_ml_api.ps1
```

**Nutzung:**
```python
import requests

# Modell laden
requests.post('http://localhost:5001/api/ml/load', json={
    'model_name': 'my_model',
    'model_path': 'models/ml/model.h5'
})

# Prediction
response = requests.post('http://localhost:5001/api/ml/predict', json={
    'model_name': 'my_model',
    'features': [[0.5, 0.3, ...]]
})
```

---

## ğŸ“Š Performance

### Tests

```powershell
# Alle ML/RL Tests ausfÃ¼hren
.\venv\Scripts\python.exe -m pytest tests/test_rl_environment.py -v
.\venv\Scripts\python.exe -m pytest tests/test_portfolio_optimizer.py -v
```

**Test Results:**
- âœ… RL Environment: 9/9 Tests passing
- âœ… Portfolio Optimizer: 11/11 Tests passing

---

## ğŸ“ Lernressourcen

### Dokumentation
- [RL_GUIDE.md](RL_GUIDE.md) - Reinforcement Learning Guide
- [ML_GUIDE.md](ML_GUIDE.md) - Machine Learning Guide
- [PORTFOLIO_OPTIMIZATION_GUIDE.md](PORTFOLIO_OPTIMIZATION_GUIDE.md) - Portfolio Theory

### Externe Ressourcen
- [Stable-Baselines3 Docs](https://stable-baselines3.readthedocs.io/)
- [OpenAI Gym](https://www.gymlibrary.dev/)
- [Optuna](https://optuna.readthedocs.io/)
- [TensorFlow](https://www.tensorflow.org/)

---

## ğŸ”§ Troubleshooting

### Import Fehler

```powershell
# Dependencies neu installieren
.\venv\Scripts\python.exe -m pip install -r requirements.txt --upgrade
```

### API startet nicht

```powershell
# Port prÃ¼fen
netstat -ano | findstr :5001

# Anderen Port verwenden
$env:ML_API_PORT = "5002"
.\scripts\start_ml_api.ps1
```

### Training langsam

```python
# GPU nutzen (falls verfÃ¼gbar)
import tensorflow as tf
print(f"GPUs available: {len(tf.config.list_physical_devices('GPU'))}")

# Batch Size erhÃ¶hen
agent.create_model(batch_size=128)  # statt 32
```

---

## ğŸš€ NÃ¤chste Schritte

1. **Eigene Daten verwenden:**
   - Historische Preis-Daten laden
   - Indikatoren berechnen
   - Features vorbereiten

2. **Modelle trainieren:**
   - RL Agent fÃ¼r Trading-Strategie
   - ML Modell fÃ¼r Signal-Prediction
   - Portfolio-Optimierung durchfÃ¼hren

3. **Hyperparameter optimieren:**
   - Optuna-Tuning durchfÃ¼hren
   - Beste Parameter speichern
   - Walk-Forward Validation

4. **Deployment:**
   - Models Ã¼ber Flask API bereitstellen
   - Integration in Live-Trading
   - Monitoring & Logging

---

## ğŸ“ Acceptance Criteria (ErfÃ¼llt)

- [x] RL-Umgebung ist einsatzbereit
- [x] DQN/PPO-Modelle trainiert und gespeichert
- [x] Optuna-Tuning implementiert
- [x] Portfolio-Optimierung fÃ¼r mehrere Assets
- [x] ML-Modell Deployment Ã¼ber API
- [x] Alle Komponenten getestet
- [x] Dokumentation vollstÃ¤ndig

---

## ğŸ™ Contributing

Siehe [CONTRIBUTING.md](CONTRIBUTING.md) fÃ¼r Details.

---

**Made for Windows â­ | PowerShell-First | DRY_RUN Default**
